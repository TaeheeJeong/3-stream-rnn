{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial\n",
    "This will just be a quick intro to the functionality of Keras. The network itself will be complete functionable and able to train as is but I encourage you to play around with it! Here is a list of possible things you could change to observe different results.\n",
    "- Number of layers\n",
    "- Size of layers\n",
    "- Types of layers\n",
    "- Optimization function\n",
    "- Error function\n",
    "- Maybe even try playing around with making the network not fully connected(can't help you with this but if you get some interesting results let me know! :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "#Import all the functionality we will need. If you are going to use\n",
    "#other layers or error functions then make sure you are importing\n",
    "#the correct modules!\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyper-Parameters\n",
    "Recall that hyperparameters are not parameters that we can train like the weights of the connections but we must define. Some examples are the number of output classes, batch sizes, epochs, layer sizes, etc.\n",
    "\n",
    "NOTE: There ARE ways to find the optimum settings of hyperparameters and is a field of study called hyperparameters optimization. Look it up if your interested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set parameters\n",
    "nb_classes = 10\n",
    "batch_size = 256\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dataset and Pre-process It\n",
    "Keras comes with the MNIST dataset built in so you can quickly test out your networks. It is also important to make sure your data is preprocessed before throwing it at a network. Some good things to do is subtracting the mean, scaling the variance, etc. Then we must make sure our data is the righ shape to give the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (60000, 1, 28, 28))\n",
      "(60000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Preprocess data\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding your Random Number Generator\n",
    "It's good practice to always seed your random number generator. This way we know that when we make a change to our network and get better results we can guaruntee that the better performance is due to the changes we made and not the inherit randomness of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Network\n",
    "I will be making 2 different networks. 1 will be a simple MLP and the other will be a convolutional neural network. Keras does allow you to use many other types of layers so I encourage you to look into more of them and try them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create network\n",
    "model = Sequential()\n",
    "\n",
    "#32 conv filters using 5x5 kernels and relu activation\n",
    "model.add(Convolution2D(32,1,5,5,border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#32 conv filters using 3x3 kernels and relu activation\n",
    "model.add(Convolution2D(32,32,3,3,border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Max pooling and 50% Dropout\n",
    "#I will explain these later\n",
    "model.add(MaxPooling2D(poolsize=(2,2)))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "#16 conv filters using 3x3 kernels and relu activation\n",
    "model.add(Convolution2D(16,32,3,3,border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#50% Dropout\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "#Flatten and add dense network\n",
    "#Dense here just means normal nueron connections. \n",
    "model.add(Flatten())\n",
    "model.add(Dense(16*196, 128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation\n",
    "The process of compiling the network is USUALLY a 1-liner. Simply tell Keras what type of loss(error) function and optimizer you want to use and that's it! If you want to use a custom loss function or optimizer then the process is a bit more complicated but is completely documented in the Keras documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOTE: Compilation will take a while! Don't fret.\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Netwok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 0\n",
      "60000/60000 [==============================] - 6s - loss: 0.4581 - acc: 0.8536 - val_loss: 0.0912 - val_acc: 0.9715\n",
      "Epoch 1\n",
      "60000/60000 [==============================] - 6s - loss: 0.1561 - acc: 0.9525 - val_loss: 0.0531 - val_acc: 0.9835\n",
      "Epoch 2\n",
      "60000/60000 [==============================] - 6s - loss: 0.1171 - acc: 0.9646 - val_loss: 0.0429 - val_acc: 0.9858\n",
      "Epoch 3\n",
      "60000/60000 [==============================] - 6s - loss: 0.0992 - acc: 0.9697 - val_loss: 0.0367 - val_acc: 0.9878\n",
      "Epoch 4\n",
      "60000/60000 [==============================] - 6s - loss: 0.0877 - acc: 0.9740 - val_loss: 0.0320 - val_acc: 0.9895\n",
      "Epoch 5\n",
      "60000/60000 [==============================] - 6s - loss: 0.0797 - acc: 0.9759 - val_loss: 0.0308 - val_acc: 0.9903\n",
      "Epoch 6\n",
      "60000/60000 [==============================] - 6s - loss: 0.0738 - acc: 0.9777 - val_loss: 0.0296 - val_acc: 0.9904\n",
      "Epoch 7\n",
      "60000/60000 [==============================] - 6s - loss: 0.0713 - acc: 0.9790 - val_loss: 0.0267 - val_acc: 0.9908\n",
      "Epoch 8\n",
      "60000/60000 [==============================] - 6s - loss: 0.0651 - acc: 0.9806 - val_loss: 0.0253 - val_acc: 0.9909\n",
      "Epoch 9\n",
      "60000/60000 [==============================] - 6s - loss: 0.0644 - acc: 0.9810 - val_loss: 0.0257 - val_acc: 0.9923\n",
      "('Test score:', 0.025674144976772367)\n",
      "('Test accuracy:', 0.99229999999999996)\n"
     ]
    }
   ],
   "source": [
    "#!!!!!!!IMPORTANT!!!!!!!!: The training process will take a LOOOOOOOONG time for the convolutional netwok.\n",
    "#If you are going to try using this network then either leave it running over night or reduce the number of\n",
    "#convolutional layers. \n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99% Accuracy! You can see that convolutional networks are VERY good at image recognition tasks. \n",
    "\n",
    "## Simple MLP Network\n",
    "I won't outline any of the specifics here as it follows the same general structure as the convolutional network. This is the best one I could come up with without convolutional layers but I encourage you to try and beat me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "nb_epoch = 20\n",
    "nb_classes = 10\n",
    "batch_size = 256\n",
    "\n",
    "#Create network\n",
    "model2 = Sequential()\n",
    "\n",
    "#Note that you can put the activation in the layer decleration\n",
    "#You can do a lot more also so check out the documentation!\n",
    "model2.add(Dense(28*28,250,activation='relu'))\n",
    "\n",
    "model2.add(Dense(250,400,activation='relu'))\n",
    "model2.add(Dropout(.5))\n",
    "\n",
    "model2.add(Dense(400,128,activation='relu'))\n",
    "\n",
    "model2.add(Dense(128, nb_classes))\n",
    "model2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 0\n",
      "60000/60000 [==============================] - 1s - loss: 0.4584 - acc: 0.8569 - val_loss: 0.2150 - val_acc: 0.9351\n",
      "Epoch 1\n",
      "60000/60000 [==============================] - 1s - loss: 0.1543 - acc: 0.9535 - val_loss: 0.1389 - val_acc: 0.9562\n",
      "Epoch 2\n",
      "60000/60000 [==============================] - 1s - loss: 0.1069 - acc: 0.9672 - val_loss: 0.2185 - val_acc: 0.9342\n",
      "Epoch 3\n",
      "60000/60000 [==============================] - 1s - loss: 0.0802 - acc: 0.9753 - val_loss: 0.0912 - val_acc: 0.9708\n",
      "Epoch 4\n",
      "60000/60000 [==============================] - 1s - loss: 0.0649 - acc: 0.9800 - val_loss: 0.0872 - val_acc: 0.9721\n",
      "Epoch 5\n",
      "60000/60000 [==============================] - 1s - loss: 0.0524 - acc: 0.9839 - val_loss: 0.0846 - val_acc: 0.9761\n",
      "Epoch 6\n",
      "60000/60000 [==============================] - 1s - loss: 0.0431 - acc: 0.9869 - val_loss: 0.1706 - val_acc: 0.9502\n",
      "Epoch 7\n",
      "60000/60000 [==============================] - 1s - loss: 0.0367 - acc: 0.9885 - val_loss: 0.0770 - val_acc: 0.9775\n",
      "Epoch 8\n",
      "60000/60000 [==============================] - 1s - loss: 0.0308 - acc: 0.9905 - val_loss: 0.0651 - val_acc: 0.9822\n",
      "Epoch 9\n",
      "60000/60000 [==============================] - 1s - loss: 0.0250 - acc: 0.9916 - val_loss: 0.0876 - val_acc: 0.9785\n",
      "Epoch 10\n",
      "60000/60000 [==============================] - 1s - loss: 0.0218 - acc: 0.9929 - val_loss: 0.0698 - val_acc: 0.9824\n",
      "Epoch 11\n",
      "60000/60000 [==============================] - 1s - loss: 0.0203 - acc: 0.9933 - val_loss: 0.0960 - val_acc: 0.9774\n",
      "Epoch 12\n",
      "60000/60000 [==============================] - 1s - loss: 0.0172 - acc: 0.9946 - val_loss: 0.0779 - val_acc: 0.9803\n",
      "Epoch 13\n",
      "60000/60000 [==============================] - 1s - loss: 0.0141 - acc: 0.9955 - val_loss: 0.0872 - val_acc: 0.9797\n",
      "Epoch 14\n",
      "60000/60000 [==============================] - 1s - loss: 0.0128 - acc: 0.9959 - val_loss: 0.0867 - val_acc: 0.9803\n",
      "Epoch 15\n",
      "60000/60000 [==============================] - 1s - loss: 0.0112 - acc: 0.9963 - val_loss: 0.0773 - val_acc: 0.9841\n",
      "Epoch 16\n",
      "60000/60000 [==============================] - 1s - loss: 0.0089 - acc: 0.9972 - val_loss: 0.0896 - val_acc: 0.9815\n",
      "Epoch 17\n",
      "60000/60000 [==============================] - 1s - loss: 0.0086 - acc: 0.9971 - val_loss: 0.0864 - val_acc: 0.9819\n",
      "Epoch 18\n",
      "60000/60000 [==============================] - 1s - loss: 0.0067 - acc: 0.9977 - val_loss: 0.0874 - val_acc: 0.9798\n",
      "Epoch 19\n",
      "60000/60000 [==============================] - 1s - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0915 - val_acc: 0.9814\n",
      "('Test score:', 0.091494554800701733)\n",
      "('Test accuracy:', 0.98140000000000005)\n"
     ]
    }
   ],
   "source": [
    "#This network should not take much time at all to train.\n",
    "model2.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model2.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
