{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning\n",
    "My hope is that this will be a good introduction to what deep learning is, what the objective of it is, how to use it, when to use it, and many of its derivative algorithms.\n",
    "\n",
    "## Perceptron\n",
    "The basic unit of a neural network is the perceptron displayed below. It takes in a variable amount of inputs, aggregates them according to sum function (could be a linear sum or something else entirely) and then produces an output depending on the output.\n",
    "![Perceptron](img191.gif)\n",
    "\n",
    "So you have an input vector $X$, a set of weights $W$ that place a weight on each $x_i \\in X$, an aggregation function $f$, and a activation function $f_i$ which determines if the perceptron will output a 1 or 0(in the binary case). \n",
    "\n",
    "The learning algorithm for a perceptron is rather simple and you can read a complete description [here](https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm). The idea is:\n",
    "\n",
    "1. Randomly initialize the weights.\n",
    "2. For 1 training example calculate $y_i = f_i (f(X*W))$.\n",
    "3. $w_inew = w_i + \\alpha * t_j-y_i$ (update the weight with the difference between the target and the actual output scaled by some learning rate alpha.\n",
    "\n",
    "## Multi-layer Perceptrons (Neural Networks)\n",
    "The basic kind of neural network is the multi-layer perceptron(MLP). Simply stated it is just a network of layers of perceptrons. An example is shown below.\n",
    "![Multi-layer Perceptron](MLFNwithWeights.jpg)\n",
    "\n",
    "Now we have a matrix $Wi$ between each layer of perceptrons with $w_{ij} \\in W$ being the weight between node $i$ in the pervious layer to node $j$ in the next. Each node still has an activation function but now we can have more non-linearity introduced as the output is now a non-linear combination of a non-linear combination of the inputs.\n",
    "\n",
    "## Backpropogation\n",
    "The training procedure for a multi-layey proceptron is considerably more complex. The process is called backpropogation. The procedure is explained EXCELENTLY [here](http://iamtrask.github.io/2015/07/12/basic-python-network/) and I highly reccomend going through the exercise of implementing it yourself. \n",
    "\n",
    "The basic idea is to perform forward propogation which is to simply run a training test case through the network and get an output. If it is incorrect then calculate the the error with respect to a particular node. This is done byu going backwards through the layers and calculating the partial derivative of the error with respect to the output of the nonlinear activation function $f_i$ of the node i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
